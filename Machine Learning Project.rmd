---
title: "Machine Learning Course Project"
author: "Bob Wothe"
date: "Friday, September 25, 2015"
output: html_document
---

##Synonpsis

One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this report, we will examine data from accelerometers on the belt, forearm, arm, and dumbell of six participants. They were asked to perform barbell lifts correctly and incorrectly in five different ways. 

We will then use this data to predict the manner in which each participant did the exercise. This report has four parts that will be listed in a fashion that allows the grader the easiest possible access to the pertinent information: 

1. How cross validation was used
2. The expected out of sample error
3. How the model was built
4. Why choices were made in this fashion

Let's jump in.

#1. How cross validation was used

As explained further below, the training data set was split into two subsamples: subTraining and subTesting (75% and 25%, respectively.) The subTraining set was initially used to fit the the model, and it is then tested on the subTesting data. At that point we will determine our most accurate model and then test it on the original Testing data set.

#2. The expected out of sample error

Using the random forest model as described in part 3 below, we can estimate that the out of sample error is just .5%. Given that the testing data set is just 20 cases, it is within reason that none of the cases would be incorrectly classified.

#3. How the model was built

Let's begin by loading the required packages:

```{r}
options(warn=-1)
library(lattice)
library(ggplot2)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot) 
set.seed(1234)
```


Now we can load the data and clean it up:

```{r}
trainingset <- read.csv("pml-training.csv", na.strings=c("NA","#DIV/0!", ""))

testingset <- read.csv('pml-testing.csv', na.strings=c("NA","#DIV/0!", ""))

dim(trainingset)
dim(testingset)

trainingset<-trainingset[,colSums(is.na(trainingset)) == 0]
testingset <-testingset[,colSums(is.na(testingset)) == 0]

trainingset   <-trainingset[,-c(1:7)]
testingset <-testingset[,-c(1:7)]

dim(trainingset)
dim(testingset)
head(trainingset)
head(testingset)

```


At this point we can create subsamples in order to cross validate.

The training data set will be split into two parts: subTraining (75%) and subTesting (25%). This split will be done randomly.

```{r}
subsamples <- createDataPartition(y=trainingset$classe, p=0.75, list=FALSE)
subTraining <- trainingset[subsamples, ] 
subTesting <- trainingset[-subsamples, ]
dim(subTraining)
dim(subTesting)
head(subTraining)
head(subTesting)
```

With the data cleaned up and split into subsamples for cross validation purposes, we can now run some prediction models.

Let's use a decision tree first:

```{r}
model1 <- rpart(classe ~ ., data=subTraining, method="class")

prediction1 <- predict(model1, subTesting, type = "class")

rpart.plot(model1, main="Classification Tree", extra=102, under=TRUE, faclen=0)

confusionMatrix(prediction1, subTesting$classe)
```

Now, we'll use a random forest:

```{r}
model2 <- randomForest(classe ~. , data=subTraining, method="class")

prediction2 <- predict(model2, subTesting, type = "class")

confusionMatrix(prediction2, subTesting$classe)
````

After reviewing the results of these two models, we can see that the random forest model is the superior choice. The random forest model was 99.5% accurate whereas the decision tree model was just 73.9%.


#4 Why choices were made in this fashion

With nearly 20,000 samplesin the data set, it was a no-brainer to remove missing values and irrelevant features -- there was still plenty of data to go on. The decision tree and random forest models are good choices for this type of modeling because each do a great job of determining which data is the most important to properly classify the results.
